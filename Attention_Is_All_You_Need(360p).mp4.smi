<sami><head><Title></Title><style type='text/css'><!--.en { Name:English (auto-generated) ;lang:en; SAMIType:CC;}--></style><SYNC Start=469><P class='en'>hi there today we&#39;re looking at<SYNC Start=3210><P class='en'>attention is all you need by Google just<SYNC Start=6690><P class='en'>to declare I don&#39;t work for Google just<SYNC Start=8760><P class='en'>because we&#39;ve been looking at Google<SYNC Start=10260><P class='en'>papers lately but it&#39;s just an<SYNC Start=12960><P class='en'>interesting paper and we&#39;re gonna see<SYNC Start=15750><P class='en'>what&#39;s the deal with it so basically<SYNC Start=18660><P class='en'>what the authors are saying is we should<SYNC Start=21300><P class='en'>kind of get away from basically onions<SYNC Start=27599><P class='en'>so traditionally what you would do and<SYNC Start=30990><P class='en'>these authors particular interested in<SYNC Start=33570><P class='en'>NLP natural language processing so<SYNC Start=35820><P class='en'>traditionally when you had like a<SYNC Start=37649><P class='en'>language task the cat eats the mouse and<SYNC Start=46340><P class='en'>you would like to translate this to say<SYNC Start=50960><P class='en'>any other language like let&#39;s say German<SYNC Start=56550><P class='en'>or whatever what you would do is you<SYNC Start=60420><P class='en'>would try to encode this sentence into a<SYNC Start=63390><P class='en'>representation and then decode it again<SYNC Start=65610><P class='en'>so somehow somehow this sentence needs<SYNC Start=70710><P class='en'>to all go into say one vector and then<SYNC Start=73830><P class='en'>this one vector needs to somehow be<SYNC Start=76860><P class='en'>transformed into the target language so<SYNC Start=80880><P class='en'>these are tradition called sack to sack<SYNC Start=82860><P class='en'>tasks and they have been solved so far<SYNC Start=87720><P class='en'>using recurrent neural networks you<SYNC Start=90><P class='en'>might know the Alice TN networks that<SYNC Start=94110><P class='en'>are very popular for these tasks what<SYNC Start=96540><P class='en'>basically happens in an RNN is that you<SYNC Start=98520><P class='en'>go over the say source sentence here one<SYNC Start=102509><P class='en'>by one here you take the word the you<SYNC Start=105149><P class='en'>kind of encode it maybe with a word<SYNC Start=108659><P class='en'>vector if you know that is so you turn<SYNC Start=111540><P class='en'>it into like a vector a word vector and<SYNC Start=114><P class='en'>then you use a neural network to turn<SYNC Start=117240><P class='en'>this vector into what we call a hidden<SYNC Start=120659><P class='en'>state so this H 0 is a hidden state you<SYNC Start=126840><P class='en'>then take the second token here cat you<SYNC Start=132120><P class='en'>again take it<SYNC Start=133349><P class='en'>world vector because need to represent<SYNC Start=136620><P class='en'>it with numbers somehow so you use word<SYNC Start=139260><P class='en'>vectors for that you turn this into you<SYNC Start=143489><P class='en'>put it through the same function so here<SYNC Start=145440><P class='en'>is what it&#39;s like a little easy for<SYNC Start=148200><P class='en'>encoder turn into the same function but<SYNC Start=150510><P class='en'>this time this hidden state also gets<SYNC Start=153900><P class='en'>plugged in here so the word vector did<SYNC Start=156120><P class='en'>instead you can actually think of having<SYNC Start=158010><P class='en'>like a started state here a start<SYNC Start=162629><P class='en'>usually people either learn this or just<SYNC Start=165690><P class='en'>initialize with zeros that kind of goes<SYNC Start=167730><P class='en'>in to the encoder function so it&#39;s<SYNC Start=169860><P class='en'>always really the same function and from<SYNC Start=174269><P class='en'>the previous hidden state and the<SYNC Start=176310><P class='en'>current word vector the encoder again<SYNC Start=179400><P class='en'>predicts another hidden state h1 and so<SYNC Start=183510><P class='en'>on so you take the next token you turn<SYNC Start=186810><P class='en'>it into a word vector you put it through<SYNC Start=189390><P class='en'>this thing the encoder function and of<SYNC Start=194280><P class='en'>course this is a lot more complicated in<SYNC Start=196349><P class='en'>actual like say an LST M that&#39;s the<SYNC Start=198690><P class='en'>basic principle behind it so you you end<SYNC Start=201630><P class='en'>up with H 2 and here you&#39;d have H 3 H 4<SYNC Start=205920><P class='en'>and the last hidden state H 4 here you<SYNC Start=209400><P class='en'>would use this in kind of exactly the<SYNC Start=211620><P class='en'>same fashion you plug it into like a<SYNC Start=214230><P class='en'>decoder let the decoder which would<SYNC Start=217319><P class='en'>output you a word D and it would also<SYNC Start=222090><P class='en'>output you a next hidden state so H 5<SYNC Start=228420><P class='en'>let&#39;s say let&#39;s just go on with the with<SYNC Start=232290><P class='en'>the listing of the states and this H 5<SYNC Start=236340><P class='en'>would again go into the decoder which<SYNC Start=238859><P class='en'>would output concert like so that&#39;s how<SYNC Start=244260><P class='en'>you would decode you basically these are<SYNC Start=246660><P class='en'>n ends what they do is they kind of take<SYNC Start=249139><P class='en'>if you look on top here they take an<SYNC Start=252389><P class='en'>input a current input and they take the<SYNC Start=255720><P class='en'>last hidden state and they compute a new<SYNC Start=259079><P class='en'>hidden state in the case of the decoder<SYNC Start=261989><P class='en'>they take the hidden state and they take<SYNC Start=266460><P class='en'>kind of<SYNC Start=266940><P class='en'>the previous usually the previous word<SYNC Start=270270><P class='en'>that you output you also feed this back<SYNC Start=272640><P class='en'>into the decoder and they will output<SYNC Start=275490><P class='en'>the next word kind of make sense so you<SYNC Start=277410><P class='en'>would guess that the hidden state kind<SYNC Start=279780><P class='en'>of encode what the sentence means and<SYNC Start=282660><P class='en'>the last word that you output you need<SYNC Start=285270><P class='en'>this because maybe for grammar right you<SYNC Start=289620><P class='en'>know what you&#39;ve just output so kind of<SYNC Start=291870><P class='en'>the next word should be based on that<SYNC Start=296640><P class='en'>of course you don&#39;t have to have to do<SYNC Start=298830><P class='en'>it exactly this way but that&#39;s kind of<SYNC Start=300510><P class='en'>what what is orleans did so attention is<SYNC Start=306870><P class='en'>a mechanism here to basically increase<SYNC Start=311250><P class='en'>the performance of the orleans so the<SYNC Start=314520><P class='en'>tension would do is in in this<SYNC Start=316890><P class='en'>particular case if we look at the<SYNC Start=320040><P class='en'>decoder here if it&#39;s trying to predict<SYNC Start=324810><P class='en'>this word for cat then or the next word<SYNC Start=330750><P class='en'>here say here it wants the next word and<SYNC Start=335690><P class='en'>[Music]<SYNC Start=337460><P class='en'>in essence the only the only h6 the only<SYNC Start=343020><P class='en'>information it really has is what the<SYNC Start=345210><P class='en'>last word was german word for cat and<SYNC Start=348270><P class='en'>what the hidden state is so if we look<SYNC Start=352260><P class='en'>at what word it actually should output<SYNC Start=354240><P class='en'>in the input sentence it&#39;s this here<SYNC Start=356669><P class='en'>eats right and if we look at kind of the<SYNC Start=360740><P class='en'>the information flow that this word has<SYNC Start=365190><P class='en'>to travel so first it needs to encode<SYNC Start=367200><P class='en'>into a word vector it needs to go<SYNC Start=369419><P class='en'>through this encoder that&#39;s the same<SYNC Start=370950><P class='en'>function for all the words so nothing<SYNC Start=373380><P class='en'>specific and we learned to the word eats<SYNC Start=375330><P class='en'>here all right let&#39;s go through this<SYNC Start=377370><P class='en'>hidden state traverse again into another<SYNC Start=380130><P class='en'>step this hidden state because we have<SYNC Start=382560><P class='en'>two more tokens and then the next state<SYNC Start=385020><P class='en'>state then it goes all the way to the<SYNC Start=387570><P class='en'>decoder where the first two words are<SYNC Start=391590><P class='en'>decoded and still so this H six this<SYNC Start=394919><P class='en'>hidden state somehow still needs to<SYNC Start=396690><P class='en'>retain the information that now the<SYNC Start=400710><P class='en'>it&#39;s somehow is kind of their world to<SYNC Start=404670><P class='en'>be translated and that they that the<SYNC Start=408390><P class='en'>decoder should find the German word for<SYNC Start=410430><P class='en'>that so that&#39;s that&#39;s of course very a<SYNC Start=416370><P class='en'>very long path or there&#39;s a lot of<SYNC Start=418350><P class='en'>transformations involved over these all<SYNC Start=421560><P class='en'>of these hidden states and the hidden<SYNC Start=423450><P class='en'>states not only do they need to remember<SYNC Start=425250><P class='en'>this particular word but all of the<SYNC Start=427170><P class='en'>words and the order and so on and the<SYNC Start=430710><P class='en'>grammar Norquay the grammar you can<SYNC Start=433560><P class='en'>actually learn with the decoders<SYNC Start=435300><P class='en'>themselves but kind of the meaning and<SYNC Start=436920><P class='en'>the structure of the sentence so it&#39;s<SYNC Start=439680><P class='en'>very hard for an RNN to learn all of<SYNC Start=442980><P class='en'>this what they what we call long-range<SYNC Start=444780><P class='en'>dependencies and so naturally you<SYNC Start=448920><P class='en'>actually think well why can&#39;t we just<SYNC Start=450870><P class='en'>you know decode the first word to the<SYNC Start=453150><P class='en'>first word the second word to the second<SYNC Start=454980><P class='en'>world it actually works pretty well in<SYNC Start=457290><P class='en'>this example right like the the cat cuts<SYNC Start=460170><P class='en'>it eats the week just decoded it one by<SYNC Start=463080><P class='en'>one about of course that&#39;s not how<SYNC Start=464730><P class='en'>translation works in translations the<SYNC Start=466620><P class='en'>sentences can become rearranged in the<SYNC Start=469050><P class='en'>target language like one word can become<SYNC Start=471750><P class='en'>many words or you could even be an<SYNC Start=474600><P class='en'>entirely different expression so<SYNC Start=477140><P class='en'>attention is a mechanism by which this<SYNC Start=479340><P class='en'>decoder here in this step that we&#39;re<SYNC Start=481260><P class='en'>looking at can actually decide to go<SYNC Start=484590><P class='en'>back and look at particular parts of the<SYNC Start=487230><P class='en'>input especially what it would do<SYNC Start=490380><P class='en'>anything like popular attention<SYNC Start=492110><P class='en'>mechanisms is that the dis decoder here<SYNC Start=495900><P class='en'>would can decide to attend to the hidden<SYNC Start=501150><P class='en'>states of the input sentence what that<SYNC Start=504570><P class='en'>means is in in this particular case we<SYNC Start=506610><P class='en'>would like to teach the decoder somehow<SYNC Start=508920><P class='en'>that AHA look here I need to pay close<SYNC Start=512580><P class='en'>attention to this step here because that<SYNC Start=516599><P class='en'>was the step when the word eats was just<SYNC Start=519539><P class='en'>encoded so it probably has a lot of<SYNC Start=522150><P class='en'>information about what I would like to<SYNC Start=525210><P class='en'>do right now namely translate this word<SYNC Start=529290><P class='en'>eats so this mechanism<SYNC Start=533940><P class='en'>if you look at the information flow it<SYNC Start=536160><P class='en'>simply it goes through this word vector<SYNC Start=538920><P class='en'>goes through one encoding step and then<SYNC Start=541710><P class='en'>is that hidden state and then the<SYNC Start=543630><P class='en'>decoder can look directly at that so the<SYNC Start=546540><P class='en'>the path length of information is much<SYNC Start=548610><P class='en'>shorter than going through all the<SYNC Start=550980><P class='en'>hidden states in a traditional way so<SYNC Start=553800><P class='en'>that&#39;s where tension helps and the way<SYNC Start=557010><P class='en'>that the decoder decides what to look at<SYNC Start=559920><P class='en'>is like a kind of an addressing scheme<SYNC Start=563430><P class='en'>you may know it from neural turing<SYNC Start=565320><P class='en'>machines or or kind of other other kind<SYNC Start=571830><P class='en'>of neural algorithms things so what the<SYNC Start=575130><P class='en'>decoder will do is in each step it would<SYNC Start=577170><P class='en'>output a bunch of keys oops sorry about<SYNC Start=582540><P class='en'>that that&#39;s my hand being trippy so what<SYNC Start=588330><P class='en'>I would output is a bunch of keys so K 1<SYNC Start=591660><P class='en'>through K and what would these keys<SYNC Start=598950><P class='en'>would do is they would index these<SYNC Start=602370><P class='en'>hidden kind of hidden states via a kind<SYNC Start=608520><P class='en'>of softmax architecture and we&#39;re gonna<SYNC Start=612120><P class='en'>look at this I think in the actual paper<SYNC Start=614790><P class='en'>we&#39;re discussing because it&#39;s gonna<SYNC Start=616890><P class='en'>become more clear which is kind of<SYNC Start=619230><P class='en'>notice that the decoder here can decide<SYNC Start=621330><P class='en'>to attend it to the input sentence and<SYNC Start=625110><P class='en'>kind of draw information directly from<SYNC Start=627><P class='en'>there instead of having to go just to<SYNC Start=630450><P class='en'>the hidden state it&#39;s provided with so<SYNC Start=633300><P class='en'>if we go to the paper here what do these<SYNC Start=636180><P class='en'>authors propose and the thing is they<SYNC Start=639870><P class='en'>teach the origins they basically say<SYNC Start=642300><P class='en'>attention is all you need you don&#39;t need<SYNC Start=644130><P class='en'>the entire recurrent things basically in<SYNC Start=646920><P class='en'>every step of this decode of this and<SYNC Start=649550><P class='en'>basically of the decoding so you want to<SYNC Start=651780><P class='en'>produce the target sentence so in this<SYNC Start=653820><P class='en'>step in this step in this step you can<SYNC Start=657560><P class='en'>basically you don&#39;t need the recurrence<SYNC Start=660570><P class='en'>even just kind of do attention over<SYNC Start=663810><P class='en'>everything and you<SYNC Start=667139><P class='en'>be fine namely what they do is they<SYNC Start=671389><P class='en'>propose this transformer architecture so<SYNC Start=674670><P class='en'>what does it do it has two parts what&#39;s<SYNC Start=678239><P class='en'>what&#39;s called an encoder and a decoder<SYNC Start=680429><P class='en'>but don&#39;t kind of be confused um because<SYNC Start=687359><P class='en'>this all happens at once so this is not<SYNC Start=689850><P class='en'>an art and it all happens at once every<SYNC Start=692160><P class='en'>all the source sentence so if we again<SYNC Start=695220><P class='en'>have the cat oops that doesn&#39;t work as<SYNC Start=700709><P class='en'>easy let&#39;s just do this this is a source<SYNC Start=704309><P class='en'>sentence and then we also have a target<SYNC Start=706980><P class='en'>sentence that maybe we&#39;ve produced two<SYNC Start=709470><P class='en'>words and we want to produce this third<SYNC Start=712350><P class='en'>word here what a produces so we would<SYNC Start=716489><P class='en'>feed the entire source sentence and also<SYNC Start=720179><P class='en'>the targets and as we produced so far to<SYNC Start=723480><P class='en'>this network namely the source sentence<SYNC Start=725970><P class='en'>would go into this part and the target<SYNC Start=729299><P class='en'>that we&#39;ve produced so far would go into<SYNC Start=731519><P class='en'>this part and this is the all combined<SYNC Start=734999><P class='en'>and at the end we get an output here at<SYNC Start=740189><P class='en'>the output probabilities that kind of<SYNC Start=743220><P class='en'>tells us the probabilities for the next<SYNC Start=745769><P class='en'>word so we can choose the top<SYNC Start=748199><P class='en'>probability and then repeat the entire<SYNC Start=751139><P class='en'>process so basically every step in<SYNC Start=754129><P class='en'>production is one training sample every<SYNC Start=758100><P class='en'>step in producing a sentence here before<SYNC Start=759720><P class='en'>with the Orang ends the entire sentence<SYNC Start=762149><P class='en'>to sentence translation is one sample<SYNC Start=764129><P class='en'>because we need to back propagate<SYNC Start=766079><P class='en'>through all of these RNA in steps<SYNC Start=767999><P class='en'>because they all happen kind of in<SYNC Start=770669><P class='en'>sequence here basically output of one<SYNC Start=774299><P class='en'>single token is one sample and then the<SYNC Start=778049><P class='en'>computation is finished the back drop<SYNC Start=779759><P class='en'>happens through everything only for this<SYNC Start=782129><P class='en'>one step there is no multi-step kind of<SYNC Start=785189><P class='en'>back propagation as in Orland and this<SYNC Start=789899><P class='en'>is kind of a paradigm shift in sequence<SYNC Start=793319><P class='en'>processing because people were always<SYNC Start=795899><P class='en'>convinced that you kind of need these<SYNC Start=798149><P class='en'>recurrent things in order to<SYNC Start=800660><P class='en'>to make good to learn these dependencies<SYNC Start=804140><P class='en'>but here they basically say Nenana<SYNC Start=805970><P class='en'>we can just do attention over everything<SYNC Start=808220><P class='en'>and little bit will actually be fine if<SYNC Start=810560><P class='en'>we just do one step projections so let&#39;s<SYNC Start=814850><P class='en'>go one by one so here with an input<SYNC Start=817610><P class='en'>embedding and say an output embedding<SYNC Start=820880><P class='en'>these these are symmetrical so basically<SYNC Start=823550><P class='en'>the tokens just get embedded with say<SYNC Start=825920><P class='en'>word vectors again then there&#39;s a<SYNC Start=828140><P class='en'>positional encoding this is kind of a<SYNC Start=829940><P class='en'>special thing where because you know<SYNC Start=833630><P class='en'>lose this kind of sequence nature of<SYNC Start=836300><P class='en'>your algorithm you kind of need to<SYNC Start=837560><P class='en'>encode where the words are that you push<SYNC Start=841040><P class='en'>through the network so the network kind<SYNC Start=842540><P class='en'>of goes AHA this is a word at the<SYNC Start=843830><P class='en'>beginning of the sentence or is the word<SYNC Start=845720><P class='en'>towards the end of the sentence so or<SYNC Start=847730><P class='en'>that it can compare to words like which<SYNC Start=850070><P class='en'>one comes first<SYNC Start=851150><P class='en'>which one comes second and you do this<SYNC Start=854530><P class='en'>it&#39;s pretty easy for the networks if you<SYNC Start=856790><P class='en'>do it with kind of these trigonometric<SYNC Start=859460><P class='en'>functioning embeddings so if I draw your<SYNC Start=862790><P class='en'>sine wave and I don&#39;t need a sine wave<SYNC Start=864590><P class='en'>of that a stop was fast and I draw you a<SYNC Start=870830><P class='en'>sine wave that is even faster maybe this<SYNC Start=874850><P class='en'>one actually sync one two three four<SYNC Start=877520><P class='en'>five doesn&#39;t matter you know what I mean<SYNC Start=880460><P class='en'>so I can encode the first world you can<SYNC Start=884180><P class='en'>encode the first position with all down<SYNC Start=887210><P class='en'>and then the second position is kind of<SYNC Start=890950><P class='en'>down down up and the third position is<SYNC Start=895970><P class='en'>kind of up down up and so on so this is<SYNC Start=899150><P class='en'>kind of a continuous way of binary<SYNC Start=902270><P class='en'>encoding of position so if I want to<SYNC Start=905><P class='en'>compare two words I can just look at all<SYNC Start=907730><P class='en'>the scales of these things and I know<SYNC Start=910070><P class='en'>how this word one word has high here and<SYNC Start=913220><P class='en'>the other word is low here so they must<SYNC Start=914780><P class='en'>be pretty far away like one must be at<SYNC Start=917300><P class='en'>the beginning and one must be at the end<SYNC Start=919270><P class='en'>if they happen to match in this long<SYNC Start=923030><P class='en'>rate long wave and they also are both<SYNC Start=925850><P class='en'>kind of low in this wave and then I can<SYNC Start=929990><P class='en'>look in this way for like oh maybe<SYNC Start=932030><P class='en'>they&#39;re close together but here<SYNC Start=933850><P class='en'>I really got the information which ones<SYNC Start=935259><P class='en'>first which was second so these are kind<SYNC Start=938589><P class='en'>of positional encodings they they&#39;re not<SYNC Start=940449><P class='en'>critical to this algorithm but they just<SYNC Start=945370><P class='en'>encode where the words are which of<SYNC Start=947170><P class='en'>course that is important and it gives<SYNC Start=950529><P class='en'>the networking a significant boost in<SYNC Start=952959><P class='en'>performance but it&#39;s not like it&#39;s not<SYNC Start=956800><P class='en'>that the meat of the thing the meat of<SYNC Start=958660><P class='en'>the thing is that now that these<SYNC Start=962079><P class='en'>encoding is go into the network&#39;s they<SYNC Start=965380><P class='en'>simply do what they call tension here<SYNC Start=969449><P class='en'>attention here and attention here so<SYNC Start=973360><P class='en'>there&#39;s kind of three kinds of attention<SYNC Start=976199><P class='en'>so basically the first attention on the<SYNC Start=978819><P class='en'>bottom left is simply attention as you<SYNC Start=981550><P class='en'>can see over the input sentence so if I<SYNC Start=985630><P class='en'>told you before you need to take this<SYNC Start=987430><P class='en'>input sentence if you look over here and<SYNC Start=989769><P class='en'>you somehow need to encode it into a<SYNC Start=992410><P class='en'>hidden representation and this now looks<SYNC Start=998079><P class='en'>much more like the picture I drew here<SYNC Start=1000120><P class='en'>in the picture I drew right at the<SYNC Start=1001350><P class='en'>beginning is that all at once I kind of<SYNC Start=1004079><P class='en'>put together this head representation<SYNC Start=1006839><P class='en'>and all you do is he used attention over<SYNC Start=1009779><P class='en'>the input sequence which basically means<SYNC Start=1011610><P class='en'>you kind of pick and choose which word<SYNC Start=1013439><P class='en'>you look at more or less so with the<SYNC Start=1017370><P class='en'>bottom right so in the the output<SYNC Start=1019380><P class='en'>sentence that you&#39;ve produced so for<SYNC Start=1020790><P class='en'>example a encoded into kind of a hidden<SYNC Start=1023130><P class='en'>state and then the third on the top<SYNC Start=1026400><P class='en'>right that&#39;s the I think that sorry I<SYNC Start=1030569><P class='en'>got interrupted so as are saying the top<SYNC Start=1033688><P class='en'>right is the most interesting part of<SYNC Start=1036390><P class='en'>the attention mechanism here where<SYNC Start=1039298><P class='en'>basically it unites the kind of encoder<SYNC Start=1043168><P class='en'>part with the kind of beak let&#39;s not it<SYNC Start=1045959><P class='en'>combines the source sentence with the<SYNC Start=1048360><P class='en'>target sentence that you&#39;ve produced so<SYNC Start=1051419><P class='en'>far so as you can see maybe here I can<SYNC Start=1057090><P class='en'>just slightly annoying but I&#39;m just<SYNC Start=1063840><P class='en'>gonna remove these kind of circles here<SYNC Start=1066419><P class='en'>so<SYNC Start=1067860><P class='en'>if you can see here there is an output<SYNC Start=1071490><P class='en'>going from the part that encodes the<SYNC Start=1074429><P class='en'>source sentence and it goes into this<SYNC Start=1077940><P class='en'>multi-head attention there&#39;s two<SYNC Start=1080220><P class='en'>connections and there&#39;s also one<SYNC Start=1082769><P class='en'>connection coming from the encoded<SYNC Start=1086700><P class='en'>output so far here and so there&#39;s three<SYNC Start=1092820><P class='en'>connections going in going into this and<SYNC Start=1095720><P class='en'>we&#39;re gonna take a look at what these<SYNC Start=1098399><P class='en'>three connections are so the the three<SYNC Start=1102419><P class='en'>connections here basically are the keys<SYNC Start=1106529><P class='en'>values and queries if you see here the<SYNC Start=1111659><P class='en'>values and the keys are what is output<SYNC Start=1116039><P class='en'>by the encoding part of the source<SYNC Start=1118620><P class='en'>sentence and the query is output by the<SYNC Start=1121889><P class='en'>encoding part of the target sentence and<SYNC Start=1125750><P class='en'>these are not only one value key in<SYNC Start=1128519><P class='en'>query so there are many and this kind of<SYNC Start=1131130><P class='en'>multi-head attention fashion so there<SYNC Start=1134070><P class='en'>are just many of them instead of one but<SYNC Start=1135960><P class='en'>you can think of and as there&#39;s just<SYNC Start=1139230><P class='en'>kind of sets so the attention computed<SYNC Start=1142649><P class='en'>here is what does it do so first of all<SYNC Start=1145230><P class='en'>it calculates a adult product of the<SYNC Start=1149820><P class='en'>keys and the queries and then it is a<SYNC Start=1153360><P class='en'>soft max over this and then it<SYNC Start=1156029><P class='en'>multiplies it by the value so what does<SYNC Start=1157740><P class='en'>this do if you thought product the keys<SYNC Start=1162480><P class='en'>and the queries what you would get is so<SYNC Start=1166380><P class='en'>as you know if you have two vectors and<SYNC Start=1169429><P class='en'>the dot there dot product basically<SYNC Start=1171690><P class='en'>gives you the angle between the vectors<SYNC Start=1175220><P class='en'>with especially in high dimensions most<SYNC Start=1178380><P class='en'>vectors going to be of kind of a 90<SYNC Start=1182610><P class='en'>degree kind of I know the Americans<SYNC Start=1185789><P class='en'>doodle the little square<SYNC Start=1189080><P class='en'>so most vectors are going to be not<SYNC Start=1191250><P class='en'>aligned very well so their dot product<SYNC Start=1193380><P class='en'>will kind of be zero ish but if a key in<SYNC Start=1197039><P class='en'>the query actually aligned with each<SYNC Start=1199260><P class='en'>other like<SYNC Start=1201150><P class='en'>if they point into the same directions<SYNC Start=1204180><P class='en'>the dot product will actually be large<SYNC Start=1207710><P class='en'>so what you can think of this as the the<SYNC Start=1211260><P class='en'>keys are kind of here the keys are just<SYNC Start=1213270><P class='en'>a bunch of vectors in space and each key<SYNC Start=1219030><P class='en'>has an Associated value so each key<SYNC Start=1222450><P class='en'>there is a kind of a table value one<SYNC Start=1226710><P class='en'>value to value three this is really<SYNC Start=1231030><P class='en'>annoying if I do this over text right so<SYNC Start=1234480><P class='en'>again here so we have a bunch of keys<SYNC Start=1237050><P class='en'>right in space and with a table with<SYNC Start=1241980><P class='en'>values and each key here corresponds to<SYNC Start=1244770><P class='en'>a value value one value to value three<SYNC Start=1247380><P class='en'>value 4 and so each key is associated<SYNC Start=1251940><P class='en'>with one of these values and then when<SYNC Start=1254400><P class='en'>we introduce a query what can it do so<SYNC Start=1257100><P class='en'>query will be a vector like this and we<SYNC Start=1261780><P class='en'>simply compute D so this is Q this is<SYNC Start=1264330><P class='en'>the query we compute the dot product<SYNC Start=1266040><P class='en'>with each of the keys and and then we<SYNC Start=1272220><P class='en'>compute a softmax over this which means<SYNC Start=1274050><P class='en'>that one key will basically be selected<SYNC Start=1278370><P class='en'>so in this case it would be probably<SYNC Start=1280500><P class='en'>this blue key here that has the biggest<SYNC Start=1283020><P class='en'>dot product with the query so this is<SYNC Start=1287160><P class='en'>key to in this in this case and the<SYNC Start=1291300><P class='en'>softmax so if you if you don&#39;t know what<SYNC Start=1293280><P class='en'>a softmax is you have you have like X 1<SYNC Start=1295860><P class='en'>2 X and B like some numbers then you<SYNC Start=1298920><P class='en'>simply do you map them to the<SYNC Start=1302520><P class='en'>exponential function each one of them<SYNC Start=1306210><P class='en'>and but also each one of them you divide<SYNC Start=1309510><P class='en'>by the sum of over over I of e to the X<SYNC Start=1314100><P class='en'>I so basically and this is a<SYNC Start=1315990><P class='en'>renormalization basically you you do the<SYNC Start=1318630><P class='en'>exponential function of the numbers<SYNC Start=1320460><P class='en'>which of course this makes the kind of<SYNC Start=1322740><P class='en'>big numbers even bigger so basically<SYNC Start=1325550><P class='en'>what you end up with is one of these<SYNC Start=1328770><P class='en'>numbers x1 through xn will become very<SYNC Start=1332190><P class='en'>big compared to the others<SYNC Start=1334500><P class='en'>and then you renormalize so basically<SYNC Start=1337050><P class='en'>one of them will be almost one and the<SYNC Start=1338970><P class='en'>other ones will be almost zero simply<SYNC Start=1341010><P class='en'>the the maximum function you can think<SYNC Start=1343260><P class='en'>of in a differentiable way I mean it<SYNC Start=1345870><P class='en'>should just want to select the biggest<SYNC Start=1347070><P class='en'>entry in this case here we kind of<SYNC Start=1350550><P class='en'>select the key that aligns most with the<SYNC Start=1352260><P class='en'>query which in this case would be key<SYNC Start=1353790><P class='en'>too and then we when we multiply this<SYNC Start=1356430><P class='en'>softmax thing with the with the values<SYNC Start=1359090><P class='en'>so this query this this inner product if<SYNC Start=1366390><P class='en'>we multiply Q with K to as an inner<SYNC Start=1370140><P class='en'>product and we take the softmax over it<SYNC Start=1376040><P class='en'>softmax what we&#39;ll do is i&#39;m going to<SYNC Start=1379230><P class='en'>draw it upwards here we&#39;re going to<SYNC Start=1380940><P class='en'>induce a distribution like this and if<SYNC Start=1385560><P class='en'>we multiply this by the value it will<SYNC Start=1387330><P class='en'>basically select value two so this is<SYNC Start=1392340><P class='en'>this is kind of an indexing scheme into<SYNC Start=1395340><P class='en'>this memory of values and this is what<SYNC Start=1399450><P class='en'>then the network uses to compute further<SYNC Start=1402420><P class='en'>things using so you see the output here<SYNC Start=1405990><P class='en'>goes into kind of more layers of the<SYNC Start=1408810><P class='en'>neural network upwards so basically what<SYNC Start=1412140><P class='en'>what you can think what does this mean<SYNC Start=1414500><P class='en'>you can think of here&#39;s the whoops deep<SYNC Start=1418140><P class='en'>I want to delete this you can think of<SYNC Start=1423120><P class='en'>this as basically the encoder of the<SYNC Start=1426750><P class='en'>source sentence right here discovers<SYNC Start=1432810><P class='en'>interesting things that looks ugly it<SYNC Start=1437370><P class='en'>discovers interesting things about the<SYNC Start=1439350><P class='en'>about the source sentence and it builds<SYNC Start=1443220><P class='en'>key value pairs and then the encoder of<SYNC Start=1447840><P class='en'>the target sentence builds the queries<SYNC Start=1450060><P class='en'>and together they give you kind of the<SYNC Start=1453690><P class='en'>next the next signal so it means that<SYNC Start=1457280><P class='en'>the network basically says here&#39;s a<SYNC Start=1460200><P class='en'>bunch of things here is a here&#39;s a bunch<SYNC Start=1462570><P class='en'>of things about the source sentence<SYNC Start=1467299><P class='en'>that you might find interesting that&#39;s<SYNC Start=1469279><P class='en'>the values and the keys are ways to<SYNC Start=1475580><P class='en'>index the values so it says here&#39;s a<SYNC Start=1478309><P class='en'>bunch of things that are interesting<SYNC Start=1480169><P class='en'>which are the values and here is how you<SYNC Start=1482450><P class='en'>would address these things which is the<SYNC Start=1484879><P class='en'>keys and then the other part of the<SYNC Start=1488629><P class='en'>network builds the queries it says I<SYNC Start=1491049><P class='en'>would like to know certain things so<SYNC Start=1495619><P class='en'>think of the value is like attributes<SYNC Start=1497690><P class='en'>like here is the name and the the kind<SYNC Start=1501980><P class='en'>of tallness and the weight of a person<SYNC Start=1503989><P class='en'>right and the keys are like that the<SYNC Start=1506840><P class='en'>actual index is like name height weight<SYNC Start=1510259><P class='en'>and then the other part of the network<SYNC Start=1513379><P class='en'>can decide what I want I actually want<SYNC Start=1516169><P class='en'>the name so my query is the name it will<SYNC Start=1518690><P class='en'>be aligned with the key name and the<SYNC Start=1521359><P class='en'>corresponding value would be the name of<SYNC Start=1523369><P class='en'>the person you would like to describe so<SYNC Start=1525950><P class='en'>that&#39;s how kind of these networks work<SYNC Start=1527720><P class='en'>together and I think it&#39;s a it&#39;s a<SYNC Start=1530450><P class='en'>pretty ingenious it&#39;s not entirely new<SYNC Start=1533509><P class='en'>because it has been done of course<SYNC Start=1534950><P class='en'>before with all the differentiable<SYNC Start=1536480><P class='en'>Turing machines and whatnot but it&#39;s<SYNC Start=1540139><P class='en'>pretty cool that this actually works and<SYNC Start=1541929><P class='en'>actually works kind of better than our<SYNC Start=1545149><P class='en'>it ends if you simply do this so they<SYNC Start=1550070><P class='en'>describe a bunch of other things here I<SYNC Start=1552769><P class='en'>I don&#39;t think they&#39;re too important<SYNC Start=1556210><P class='en'>basically that the point they make about<SYNC Start=1558830><P class='en'>this attention is that it reduces path<SYNC Start=1561259><P class='en'>lengths and kind of that&#39;s the the main<SYNC Start=1563720><P class='en'>reason why it should work better with<SYNC Start=1567919><P class='en'>this entire attention mechanism you<SYNC Start=1570980><P class='en'>reduce the amount of computation steps<SYNC Start=1573649><P class='en'>that information has to flow from one<SYNC Start=1576109><P class='en'>point in the network to another and that<SYNC Start=1579190><P class='en'>what brings the major improvement<SYNC Start=1581690><P class='en'>because all the computation steps can<SYNC Start=1583999><P class='en'>make you lose information and you don&#39;t<SYNC Start=1586730><P class='en'>want that you want short path lengths<SYNC Start=1588679><P class='en'>and so that&#39;s that&#39;s what this method<SYNC Start=1591289><P class='en'>achieves and they claim that&#39;s why it&#39;s<SYNC Start=1594619><P class='en'>better and it works so well so they have<SYNC Start=1599420><P class='en'>experiments you can look at them they&#39;re<SYNC Start=1601560><P class='en'>really good at everything of course of<SYNC Start=1605490><P class='en'>course you&#39;re always have state of the<SYNC Start=1607350><P class='en'>art and I think I will conclude here if<SYNC Start=1613530><P class='en'>you want to check it out yourself<SYNC Start=1615870><P class='en'>they have extensive code on github where<SYNC Start=1618780><P class='en'>you can build your own transformer<SYNC Start=1619950><P class='en'>networks and with that have a nice day<SYNC Start=1624240><P class='en'>and see ya</body></sami>